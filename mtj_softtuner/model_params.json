{
  "GPT-Neo-125M": {
    "tokenizer_id": "gpt2",
    "newlinemode": "n",
    "params": {
      "compat": "neo",
      "layers": 12,
      "d_model": 768,
      "n_heads": 12,
      "n_vocab": 50257,
      "n_vocab_padding": 143,
      "norm": "layernorm",
      "pe": "fixed",
      "seq": 2048,
      "cores_per_replica": 4,
      "eos_token": [50256],
      "max_batch_size": 2048
    }
  },
  "GPT-Neo-2.7B": {
    "tokenizer_id": "gpt2",
    "newlinemode": "n",
    "params": {
      "compat": "neo",
      "layers": 32,
      "d_model": 2560,
      "n_heads": 20,
      "n_vocab": 50257,
      "n_vocab_padding": 143,
      "norm": "layernorm",
      "pe": "fixed",
      "seq": 2048,
      "cores_per_replica": 4,
      "eos_token": [50256],
      "max_batch_size": 2048
    }
  },
  "GPT-Neo-1.3B": {
    "tokenizer_id": "gpt2",
    "newlinemode": "n",
    "params": {
      "compat": "neo",
      "layers": 24,
      "d_model": 2048,
      "n_heads": 16,
      "n_vocab": 50257,
      "n_vocab_padding": 143,
      "norm": "layernorm",
      "pe": "fixed",
      "seq": 2048,
      "cores_per_replica": 8,
      "eos_token": [50256],
      "max_batch_size": 2048
    }
  },
  "fairseq-dense-125M": {
    "tokenizer_id": "KoboldAI/fairseq-dense-125M",
    "newlinemode": "s",
    "params": {
      "compat": "fairseq_lm",
      "n_vocab": 50261,
      "n_vocab_padding": 943,
      "norm": "layernorm",
      "pe": "fairseq_sinusoidal",
      "seq": 2048,
      "layers": 12,
      "d_model": 768,
      "n_heads": 12,
      "cores_per_replica": 6,
      "eos_token": [50259, 50259],
      "max_batch_size": 2048
    }
  },
  "fairseq-dense-355M": {
    "tokenizer_id": "KoboldAI/fairseq-dense-125M",
    "newlinemode": "s",
    "params": {
      "compat": "fairseq_lm",
      "n_vocab": 50261,
      "n_vocab_padding": 939,
      "norm": "layernorm",
      "pe": "fairseq_sinusoidal",
      "seq": 2048,
      "layers": 24,
      "d_model": 1024,
      "n_heads": 16,
      "cores_per_replica": 8,
      "eos_token": [50259, 50259],
      "max_batch_size": 2048
    }
  },
  "fairseq-dense-760M": {
    "tokenizer_id": "KoboldAI/fairseq-dense-125M",
    "newlinemode": "s",
    "params": {
      "compat": "fairseq_lm",
      "n_vocab": 50261,
      "n_vocab_padding": 939,
      "norm": "layernorm",
      "pe": "fairseq_sinusoidal",
      "seq": 2048,
      "layers": 24,
      "d_model": 1536,
      "n_heads": 16,
      "cores_per_replica": 8,
      "eos_token": [50259, 50259],
      "max_batch_size": 2048
    }
  },
  "fairseq-dense-1.3B": {
    "tokenizer_id": "KoboldAI/fairseq-dense-125M",
    "newlinemode": "s",
    "params": {
      "compat": "fairseq_lm",
      "n_vocab": 50261,
      "n_vocab_padding": 939,
      "norm": "layernorm",
      "pe": "fairseq_sinusoidal",
      "seq": 2048,
      "layers": 24,
      "d_model": 2048,
      "n_heads": 32,
      "cores_per_replica": 8,
      "eos_token": [50259, 50259],
      "max_batch_size": 2048
    }
  },
  "fairseq-dense-2.7B": {
    "tokenizer_id": "KoboldAI/fairseq-dense-125M",
    "newlinemode": "s",
    "params": {
      "compat": "fairseq_lm",
      "n_vocab": 50261,
      "n_vocab_padding": 939,
      "norm": "layernorm",
      "pe": "fairseq_sinusoidal",
      "seq": 2048,
      "layers": 32,
      "d_model": 2560,
      "n_heads": 32,
      "cores_per_replica": 8,
      "eos_token": [50259, 50259],
      "max_batch_size": 2048
    }
  },
  "fairseq-dense-6.7B": {
    "tokenizer_id": "KoboldAI/fairseq-dense-125M",
    "newlinemode": "s",
    "params": {
      "compat": "fairseq_lm",
      "n_vocab": 50261,
      "n_vocab_padding": 939,
      "norm": "layernorm",
      "pe": "fairseq_sinusoidal",
      "seq": 2048,
      "layers": 32,
      "d_model": 4096,
      "n_heads": 32,
      "cores_per_replica": 8,
      "eos_token": [50259, 50259],
      "max_batch_size": 2048
    }
  },
  "fairseq-dense-13B": {
    "tokenizer_id": "KoboldAI/fairseq-dense-125M",
    "newlinemode": "s",
    "params": {
      "compat": "fairseq_lm",
      "n_vocab": 50261,
      "n_vocab_padding": 939,
      "norm": "layernorm",
      "pe": "fairseq_sinusoidal",
      "seq": 2048,
      "layers": 40,
      "d_model": 5120,
      "n_heads": 40,
      "cores_per_replica": 8,
      "eos_token": [50259, 50259],
      "max_batch_size": 450
    }
  },
  "GPT-J-6B": {
    "tokenizer_id": "gpt2",
    "newlinemode": "n",
    "params": {
      "layers": 28,
      "d_model": 4096,
      "n_heads": 16,
      "n_vocab": 50257,
      "n_vocab_padding": 143,
      "norm": "layernorm",
      "pe": "rotary",
      "pe_rotary_dims": 64,
      "seq": 2048,
      "cores_per_replica": 8,
      "eos_token": [50256],
      "max_batch_size": 2048
    }
  },
  "pythia-12b": {
    "tokenizer_id": "EleutherAI/pythia-14m",
    "newlinemode": "n",
    "params": {
      "hidden_size": 5120,
      "initializer_range": 0.02,
      "intermediate_size": 20480,
      "layer_norm_eps": 1e-05,
      "max_position_embeddings": 2048,
      "model_type": "gpt_neox",
      "num_attention_heads": 40,
      "num_hidden_layers": 36,
      "rotary_emb_base": 10000,
      "rotary_pct": 0.25,
      "tie_word_embeddings": false,
      "vocab_size": 50688
    }
}
